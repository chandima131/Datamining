{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8XBX4Js-iLcT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzSjupC-rlzm"
   },
   "source": [
    "# Objective of the task is to predict the Profit of the state based on the other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UJgt7Pe4hBL9"
   },
   "outputs": [],
   "source": [
    "# Your code to import numpy\n",
    "# Your code to import panda\n",
    "# Your code to import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-PDHsuSxicT-"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '50_Startups.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Your code to read file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Your code to print sample data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50_Startups.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m dataset\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '50_Startups.csv'"
     ]
    }
   ],
   "source": [
    "# Your code to read file\n",
    "# Your code to print sample data\n",
    "\n",
    "dataset =pd.read_csv(\"50_Startups.csv\")\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27F819fqr0MK"
   },
   "source": [
    "# Separate the independednt and Dependent variables.\n",
    "# Profit is the dependedent variable\n",
    "\n",
    "## What is the role of the axis and inplace in following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUXxD-gViltR"
   },
   "outputs": [],
   "source": [
    "y = np.asarray(dataset['Profit'].values.tolist())\n",
    "\n",
    "dataset.drop([\"Profit\"], axis = 1, inplace = True)\n",
    "\n",
    "#to drop column which has null value. if axis = 0 it drop rows with null values\n",
    "#The inplace parameter, when set to True , allows you to drop the rows or columns without returning a new DataFrame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8d-0FfnsN0O"
   },
   "source": [
    "# We will convert  Categorical Variable values \"State\" to numbers with the One Hot Encoding Technique\n",
    "\n",
    "# Initially, we will counts value that the feature \"State\" can take.\n",
    "\n",
    "# Remember .value_counts() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3CB3Ct9i4an"
   },
   "outputs": [],
   "source": [
    "dataset.iloc[:,3].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WSSeKczss1U"
   },
   "source": [
    "# Replacing the three states by the numbers 1, 2, 3.\n",
    "# This is also called as the LabelEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RonZ0XJVi7VO"
   },
   "outputs": [],
   "source": [
    "dataset.replace(to_replace=[\"California\",\"New York\", \"Florida\"], value=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQungUWgtBIL"
   },
   "source": [
    "# We will create 3 more columns for the three states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEz2FMUfi9gK"
   },
   "outputs": [],
   "source": [
    "dataset[\"California\"] = dataset.iloc[:, 3]\n",
    "dataset[\"New York\"] = dataset.iloc[:,3]\n",
    "dataset[\"Florida\"] = dataset.iloc[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unUvog8WtNpu"
   },
   "source": [
    "# Check how the dataset looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98teOtjzjIp2"
   },
   "outputs": [],
   "source": [
    "# Your code to print few samples of dataset.\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoLuUTSBtVZj"
   },
   "source": [
    "# Performing one hot encoding for the column Calfornia.\n",
    "## Replace text 'California' with 1 and others with 0 for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZABX9aatjLdw"
   },
   "outputs": [],
   "source": [
    "dataset.loc[dataset[\"California\"]!=\"California\", \"California\"] = 0\n",
    "dataset.loc[dataset[\"California\"]==\"California\", \"California\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rrqbptyntrgw"
   },
   "source": [
    "# Perform the one hot encoding for New York and the Florida column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zcJTPVxjPYk"
   },
   "outputs": [],
   "source": [
    "# Your code to perform one hot encoding for New York column.\n",
    "dataset.loc[dataset[\"New York\"]!=\"New York\", \"New York\"] = 0\n",
    "dataset.loc[dataset[\"New York\"]==\"New York\", \"New York\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnoCxjbejSmN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Your code to perform one hot encoding for Florida column.\n",
    "\n",
    "dataset.loc[dataset[\"Florida\"]!=\"Florida\", \"Florida\"] = 0\n",
    "dataset.loc[dataset[\"Florida\"]==\"Florida\", \"Florida\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of9U6eX4uGwj"
   },
   "source": [
    "# Now look at the dataset\n",
    "## Can you find the state name from numbers in last three columns only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JUs_zpLjV_e"
   },
   "outputs": [],
   "source": [
    "# Your code to print sample\n",
    "dataset1 = dataset.iloc[:,-3:]\n",
    "dataset1.head()\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dm80EXTuX8w"
   },
   "source": [
    "# Drop the State and one of the three states column.\n",
    "## Question - Why we are dropping one column for the state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAFgpUrmjZbp"
   },
   "outputs": [],
   "source": [
    "# Your code to drop State and Florida column\n",
    "dataset = dataset.drop(['State', 'Florida'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4kE329ku90q"
   },
   "source": [
    "# Lets's check the dataset again.\n",
    "## Question - Can you guess the name of three states from the numbers (1/0) in last two column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JtxL7LPj-Ze"
   },
   "outputs": [],
   "source": [
    "# Your code to print sample dataset.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIQepd2JvPmH"
   },
   "source": [
    "# Let's further process independent variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuLd1d0ukA_E"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = np.asarray(dataset.values.tolist()).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jbjZBzrwPWM"
   },
   "source": [
    "# Print the shapes of the aray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWMcIEalkHJp"
   },
   "outputs": [],
   "source": [
    "# Your code to print the shape of X\n",
    "# Your code to print the shape of y\n",
    "\n",
    "print(\"Shape of x:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mkuT1JvwXQS"
   },
   "source": [
    "# Change the shape of the dependent variable to (len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkOIedKLkJsS"
   },
   "outputs": [],
   "source": [
    "# Your code to change the shape of the y to (len(y), 1)\n",
    "y.reshape(len(y),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "086Arswgwifr"
   },
   "source": [
    "# Perform feature scaling\n",
    "## We will study this in detail in class.\n",
    "## For now, guess what following code is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xY_jX90kqWe"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(X.shape[1]-2):\n",
    "    X[:,i] = (X[:,i] - int(np.mean(X[:,i])))/np.std(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5X_d-T-k2_f"
   },
   "outputs": [],
   "source": [
    "y = (y - int(np.mean(y)))/np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDibPJ8Xw6qJ"
   },
   "source": [
    "### Adding the feature X0 = 1, so we have the equation: y =  theta[0] * X0 + theta[1] * X1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IJwREnmk53K"
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X,np.ones((50,1))), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xeqVGkmxNqW"
   },
   "source": [
    "# Let's see X, y and their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F72giE7Zk8Xc"
   },
   "outputs": [],
   "source": [
    "# Your code to print sample values in X\n",
    "\n",
    "print(\"Shape of x:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiCADV4Xk-40"
   },
   "outputs": [],
   "source": [
    "# Your code to print the sample values in y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTjU0Q-cxser"
   },
   "source": [
    "# Let's assign the X to a variable Independedent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdWZQ-b5lDJa"
   },
   "outputs": [],
   "source": [
    "Indpendent_Variables = pd.DataFrame(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gMeaKTBx4-d"
   },
   "source": [
    "# Print Independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4ZT1IjrlNHC"
   },
   "outputs": [],
   "source": [
    "# Your code to print independent variable.\n",
    "print(Indpendent_Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6id_qS-wx_PY"
   },
   "source": [
    "# Following function splits the data into two sets - trainset and testset.\n",
    "# We can also do the same operation using  train_test_split method available in sklearn.model_selection.\n",
    "# But, let's create our own method to split data.\n",
    "\n",
    "# In the following code, set radom_state as your SID.\n",
    "\n",
    "## Comment each line of the code about what its doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyUnKqYklPXO"
   },
   "outputs": [],
   "source": [
    "SID=2224089\n",
    "def split_data(X, y, test_size = 0.2, random_state = SID):  # Set variable random_state as your SID\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    data_test_size = int(X.shape[0] * test_size)\n",
    "\n",
    "    train_indices = indices[data_test_size:]\n",
    "    test_indices = indices[:data_test_size]\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    # Your code to return X_train, y_train, X_test, y_test\n",
    "    return X_train, y_train, X_test,y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SZXGbPozKhg"
   },
   "source": [
    "#Comment each line of the following code about what its doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIoOKmLMz7SG"
   },
   "outputs": [],
   "source": [
    "def forward(X, y, theta):\n",
    "\n",
    "    y_pred = np.sum(theta * X) # provides summation of all teta*X values\n",
    "    loss = ((y_pred-y)**2)/2    #This is also the cost function\n",
    "\n",
    "    # Your code to return lost and predicted values.\n",
    "    return y_pred,loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ2et9uJ3o4i"
   },
   "source": [
    "# Comment each line of the code about what its doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOrAJ454z8qk"
   },
   "outputs": [],
   "source": [
    "def updateTheta(X, y_pred, y_true, theta, alpha, index): # function to update yhe theta values\n",
    "\n",
    "    theta -= alpha *X[index]*(y_pred-y_true[index]) # theta = theta - alpha *X[index]*(y_pred-y_true[index]) it means it provides minus slope\n",
    "    \n",
    "    # Your code to return theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "446lDTRr3vA8"
   },
   "source": [
    "## Comment each line of the code about what its doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGC0RCK00TPF"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, epochs = 10, alpha = 0.001, random_state=0): #gradient_decent function\n",
    "\n",
    "    num_rows = X.shape[0] # assign array[0] to rows variable\n",
    "    num_cols = X.shape[1] # assign array[1] to column variable\n",
    "\n",
    "    theta = np.random.randn(1,num_cols) / np.sqrt(num_rows) # assign teta values\n",
    "    train_loss = [] #define new array called train-loss\n",
    "    num_epochs = [] #define new array called num_epochs\n",
    "    train_indices = [i for i in range(X.shape[0])] #assign X.shape([0]) values to the train_indices variable\n",
    "\n",
    "    for j in range(epochs):\n",
    "        cost=0\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(train_indices)\n",
    "\n",
    "        for i in train_indices:\n",
    "            loss, y_pred = forward(X[i],y[i],theta)\n",
    "            cost+=loss\n",
    "            theta = updateTheta(X,y_pred,y,theta,alpha,i)\n",
    "\n",
    "        train_loss.append(cost)\n",
    "        num_epochs.append(j)\n",
    "\n",
    "    return theta, train_loss, num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igWtd1Lo3-ep"
   },
   "source": [
    "## Comment each line of the code about what its doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrEpGXdN0Y2q"
   },
   "outputs": [],
   "source": [
    "def test(X_test, y_test, theta_updated):\n",
    "\n",
    "    test_pred = []\n",
    "    test_loss = []\n",
    "\n",
    "    test_indices = [i for i in range(X_test.shape[0])]\n",
    "\n",
    "    for i in test_indices:\n",
    "        loss, y_test_pred = forward(X_test[i],  y_test[i], theta_updated)\n",
    "        test_pred.append(y_test_pred)\n",
    "        test_loss.append(loss)\n",
    "\n",
    "\n",
    "    # Your code to return predictions and loss\n",
    "    return test_pred,test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od4-qZRv4AAf"
   },
   "source": [
    "## Comment each line of the code about what its doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORSib6yN0dp5"
   },
   "outputs": [],
   "source": [
    "def predict(theta_updated, X_sample):\n",
    "\n",
    "    prediction = np.sum(theta_updated * X_sample)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrL_oqSeIYO0"
   },
   "source": [
    "# Create a good plot Epochs vs loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiryfU2C184a"
   },
   "outputs": [],
   "source": [
    "def plotLoss(loss, epochs):\n",
    "\n",
    "    # Your code to plot epochs vs loss\n",
    "    # Your code to print x label in the graph\n",
    "    # Your code to print y label in the graph\n",
    "    # Your code to provide title to the plot.\n",
    "    # Your code to show the plot.\n",
    "    plt.scatter (x=epoches, y=loss )\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"good plot Epochs vs loss\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "plt.scatter (x=prediction, y=epochs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-MpfkCR4IAb"
   },
   "source": [
    "# Calling the method  split_data to get train and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpxMECnipNSd"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD-K1DXW4yjP"
   },
   "source": [
    "# Call the gradient descent function with the number of epochs and learning rate of your choice. Keep number of epochs greater that 200 and learning rate less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6_XGsxmqKEs"
   },
   "outputs": [],
   "source": [
    "theta_updated, train_loss, num_epochs = # Your code to call gradient descent for training data; you need to set epoch (greater than 200) and learning rate (less than 0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLEaYqXx5PEx"
   },
   "source": [
    "# Test your regression model using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTg11ggFqM6A"
   },
   "outputs": [],
   "source": [
    "# Your code to test the model on test data and updated theta values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYpIBWt05ZdV"
   },
   "source": [
    "# Plot the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq4Ld1cTqQCX"
   },
   "outputs": [],
   "source": [
    "# Your code to plot epochs vs loss. Call the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eA7JcJ05i3t"
   },
   "source": [
    "Perform the predictions on X_tset. Call predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwMj-gm05jqK"
   },
   "outputs": [],
   "source": [
    "# Your code to predict the profit i.e. y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QX9qvbBJ5xI"
   },
   "source": [
    "# **Imortant - Lab Logbook requirement:**\n",
    "\n",
    "# Please document the following in your lab logbook:\n",
    "\n",
    "# 1. Plot the loss function.\n",
    "# 2. Record the output of all the predictions on the test data; i.e., all the predicted y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFVI2Qkt6tJ4"
   },
   "source": [
    "# **Optional Part:**\n",
    "# The rest of this notebook is Optional. It is recommended for you to complete it. However, if you have not marks would not be deducted.\n",
    "\n",
    "# Fitting the model using sklearn and comparing with our model.\n",
    "\n",
    "# Following piece of code is uncommented. Please comment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhXzFwONqUCV"
   },
   "outputs": [],
   "source": [
    "# Your code to import train_test_split from sklearn.model_selection\n",
    "# Your code to import LinearRegression from sklearn.linear_model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4JcuB2Lqby_"
   },
   "outputs": [],
   "source": [
    "dataset_sk = pd.read_csv('50_Startups.csv')\n",
    "X_sk = dataset_sk.iloc[:, :-1].values\n",
    "y_sk = dataset_sk.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3npV2v0qgKD"
   },
   "outputs": [],
   "source": [
    "labelencoder_X_sk = LabelEncoder()\n",
    "X_sk[:,3] = labelencoder_X_sk.fit_transform(X_sk[:,3])\n",
    "\n",
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_sk_categorical = onehotencoder.fit_transform(X_sk[:,3].reshape(-1,1)).toarray()\n",
    "X_sk = np.concatenate((X_sk,X_sk_categorical),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mwGdW42qn3D"
   },
   "outputs": [],
   "source": [
    "X_sk = X_sk[:, [0,1,2,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1l5re-Qqp_o"
   },
   "outputs": [],
   "source": [
    "X_sk.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx5w-R0lqs2Y"
   },
   "outputs": [],
   "source": [
    "# Your code to perform train test split with 20% data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtPNcs-9qvPo",
    "outputId": "95195c0f-0486-41f7-aa25-39457f73bd70"
   },
   "outputs": [],
   "source": [
    "regressor_sk = LinearRegression()\n",
    "regressor_sk.fit(X_train_sk, y_train_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDTaghDbqyV4"
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred = regressor_sk.predict(X_test_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQ6Xu0uOq0uf"
   },
   "outputs": [],
   "source": [
    "X_train_sk.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE6llxNiq30S"
   },
   "outputs": [],
   "source": [
    "#Making the Prediction using Sklearn Regression\n",
    "print(regressor_sk.predict([[160000,140000,5000000,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90xW5Wcrq6fc"
   },
   "outputs": [],
   "source": [
    "#Making a Prediction\n",
    "pred = predict(theta_updated,[160000,140000,5000000,1,0,1])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCcFXPhO9yYD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
